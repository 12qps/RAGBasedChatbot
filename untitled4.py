# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y-ORMyZBkdQOJpNUoM2bh-PwbTJ3rLt5
"""



!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display
from IPython.display import Markdown



def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = userdata.get("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "HUGGINGFACEHUB_API_TOKEN"

loader = PyPDFDirectoryLoader("/content/sample_data/Data")
docs = loader.load()

from google.colab import drive
drive.mount('/content/drive')

docs

text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

len(chunks)

chunks[0]

chunks[1]

chunks[2]

chunks[3]

chunks[4]

chunks[5]

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

vectorstore = Chroma.from_documents(chunks, embeddings)

query = "What is fever?"
search = vectorstore.similarity_search(query)

to_markdown(search[0].page_content)

retriever = vectorstore.as_retriever(
    search_kwargs={'k': 5}
)

retriever.get_relevant_documents(query)

from google.colab import drive

drive.mount('/content/drive')

llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/MediMate/MediMate Model/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1)

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are personal medical assistant called MediMate developed by Navaz that follows instruction extremely well.
Please be truthful and give direct answers.You can give health advice and insights.
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever,  "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

response = rag_chain.invoke("What is heart attack??")

to_markdown(response)

import sys

while True:
  user_input = input(f"Input Prompt: ")
  if user_input == 'exit':
    print('Exiting')
    sys.exit()
  if user_input == '':
    continue
  result = rag_chain.invoke(user_input)
  print("Answer: ",result)